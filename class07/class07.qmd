---
title: "Class 7: Machine Learning 1"
author: "Alfred Phyo (PID: A18129346)"
format: pdf
toc: TRUE
---

## Background

Today we will begin our exploration of important machine learning methods with a focus on **clustering** and **dimensionallity reduction**.

To start testing these methods let's make up some sample data to cluster where we know what the answer should be.


```{r}
hist(rnorm(3000, mean = 10))
```
> Q. Can you generate 30 numbers centered at +3 taken at random from a normal distribution?

```{r}
tmp <- c(rnorm(30, mean=3), rnorm(30, mean = -3))

x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

## K-means clustering

The main function in "base R" for K-means clustering is called `kmeans()`, let's try it out:

```{r}
k <- kmeans(x, centers = 2)
k
```
> Q. What component of your kmeans result object has the cluster centers?


```{r}
k$centers
```
> Q. What component of your kmeans result object has the cluster size (i.e. how many points are in each cluster)?


```{r}
k$size
```


> Q. What component of your kmeans result object has the cluster membership vector (i.e. the main clustering result: which points are in which cluster)?

```{r}
k$cluster
```


> Q. Plot the results of clustering (i.e. our data colored by the clustering result) along with the cluster centers. 

```{r}
plot(x, col = k$cluster)
points(k$centers, col = "blue", pch=15, cex = 2)
```


> Q. Can you run `kmeans()` again and cluster `x` into 4 clusters and plot the results just like we did above with coloring by cluster and the cluster centers shown in blue?

```{r}
k4 <- kmeans(x, centers = 4)
plot(x, col = k4$cluster)
points(k4$centers, col = "blue", pch=15, cex = 2)
```

> **Key-point:** Kmeans will always return the clustering that we ask for (this is the "K" or "centers" in K-means)!

```{r}
k$tot.withinss
```

## Hierarchical clustering

The main function for hierarchical clustering in base R is called `hclust()`. One of the main differences with respect to the `kmeans()` function is that you can not just pass your input data directly to `hclust()` - it needs a "distance matrix" as input. We can get this from lot's of places including the `dist()` function.

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```


We can "cut" the dendrogram or "tree" at a given height to yield our "clusters". For this we use the function `cutree()`

```{r}
plot(hc)
abline(h=10, col="red")
grps <- cutree(hc, h=10)
```

```{r}
grps
```

> Q. Plot our data `x` colored by the clustering result from `hclust()` and `cutree()`?

```{r}
plot(x, col = grps)
```

Demonstrating how cutting at different heights yield different membership vectors.
```{r}
plot(hc)
abline(h=4.2, col="red")
cutree(hc, h=4.2)
```

## Principal Component Analysis (PCA)

PCA is a popular dimensionality reduction technique that is widely used in bioinformatics.

### PCA of UK food data

Read data on food consumption in the UK

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

It looks like the row names are not set properly. We can fix this 

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
x
```

A better way to do this is fix the row names assignment at import time:

```{r}
x <- read.csv(url, row.names = 1)
x
```
> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```
There are 17 rows and 4 columns in the data frame named x. I used the `dim()` function.

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer using the `rownames()` function because it is easy to use. Setting the row names at the time of import (second approach) rather than after by using `x <- x[,-1]` (first approach) is more robust because the first approach deletes a column every time you run the code. 

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```


> Q3: Changing what optional argument in the above barplot() function results in the following plot?

Changing the beside argument to false in the above `barplot()` function results in the following plot.

> Q4. Is missing!

> Q5: We can use the pairs() function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```
Points that lie on the diagonal are of the same value. Points that are off the diagonal means they are different foods, meaning they are in different rows. If it's above the diagonal that means its on the England axis whereas it's on the Ireland axis if it's below the diagonal.

### Heatmap

We can install the **pheatmap** package with the `install.packages()` command that we used previously. Remember that we always run this in the console and not a code chunk in our quarto document.

```{r}
library(pheatmap)

pheatmap( as.matrix(x) )
```

Of all these plot really only the `pairs()` plot was useful. This however took a bit of work to interpret and will not scale when I am looking at much bigger datasets.

## PCA the rescue

The main function in "base R" for PCA is called `prcomp()`

```{r}
pca <- prcomp(t(x))
summary(pca)
```

> Q. How much vavrance is captured in the first PC?

67.4%

> Q. How many PCs do I need to capture at least 90% of the total varance in the dataset?

I need to capture 2 PCs to result in 96.5% of total varance.

> Q. Plot our main PCA result. Folks can call this different things depending on their field of study e.g. "PC plot", "ordienation plot", "Score plot", "PC1 vs PC2 plot"...

```{r}
attributes(pca)
```
To generate our PCA score plot we wannt the `pca$X` component of the result object

```{r}
pca$x
```

Base R plot
```{r}
my_cols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col = my_cols, pch =16)
```
```{r}
library(ggplot2)

ggplot(pca$x)+
  aes(PC1, PC2)+
  geom_point(col=my_cols)
```

## Digging deeper (variable loadings)

How do the original variables (i.e. the 17 different foods) contribute to our new PCs?

```{r}
ggplot(pca$rotation) +
  aes(x = PC1, 
      y = reorder(rownames(pca$rotation), PC1)) +
  geom_col(fill = "steelblue") +
  xlab("PC1 Loading Score") +
  ylab("") +
  theme_bw() +
  theme(axis.text.y = element_text(size = 9))
```

